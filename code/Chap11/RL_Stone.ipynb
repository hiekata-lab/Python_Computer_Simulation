{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2人の意思決定（石取りゲームの学習）<br>\n",
    "# Decison-Making by Two People (Learning of Stone Picking Game)<br>\n",
    "\n",
    "・強化学習の1つであるQラーニングを用いる<br>\n",
    "・This time, we use Q-learning, which is one of the reinforcement learning.<br>\n",
    "<br>\n",
    "・エージェントは2つ用いる<br>\n",
    "・We use two agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数の設定 Settings of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOTTLE_N = 11#石の数 the number of stones\n",
    "\n",
    "#Q値の初期化 Q-value initialization\n",
    "QV0=np.zeros((BOTTLE_N+1,3), dtype=np.float32)\n",
    "QV1=np.zeros((BOTTLE_N+1,3), dtype=np.float32)\n",
    "QVs = [QV0, QV1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態遷移：状態を変化させるための関数<br>\n",
    "State transiton: The function used to change state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(action, state, turn):\n",
    "    state = state + action + 1\n",
    "    rewards = [0,0]\n",
    "    done = False\n",
    "    if (state>=BOTTLE_N):\n",
    "        state = BOTTLE_N\n",
    "        rewards[turn] = -1\n",
    "        rewards[(turn+1)%2] = 1\n",
    "        done = True\n",
    "    return state, rewards, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行動選択：Q値から次の行動を選択するための関数<br>\n",
    "Select actions: The function to select next action from Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(state, epsilon, qv):\n",
    "    #徐々に最適行動のみをとる、ε-greedy法\n",
    "    #Gradually take only the optimal action at that time, ε-greedy method\n",
    "    if epsilon > np.random.uniform(0, 1):\n",
    "        next_action = np.random.choice([0, 1, 2])\n",
    "    else:\n",
    "        a = np.where(qv[state]==qv[state].max())[0]\n",
    "        #その時点での最適行動が複数ある場合はランダムに選択\n",
    "        #If there are multiple optimal actions at that time, select randomly from them\n",
    "        next_action = np.random.choice(a)\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q値の更新：状態，行動，報酬，次の状態を用いてQ値を更新するための関数<br>\n",
    "Update Q-value: The function to update Q-value based on the current state, the behavior, the reward and the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQValue(action, reward, state, state_old, qv, gamma, alpha):\n",
    "    maxQ = np.max(qv[state])\n",
    "    qv[state_old][action] = (1-alpha)*qv[state_old][action]+alpha*(reward + gamma*maxQ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数の設定 Settings of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100#エピソード数 number of episodes\n",
    "gamma = 0.9#割引率 discount rate\n",
    "alpha = 0.5#学習率 learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の実行 Do reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0 Win!!, 1 Lose\n",
      "1 : 0 Win!!, 1 Lose\n",
      "2 : 0 Lose, 1 Win!!\n",
      "3 : 0 Lose, 1 Win!!\n",
      "4 : 0 Win!!, 1 Lose\n",
      "5 : 0 Win!!, 1 Lose\n",
      "6 : 0 Lose, 1 Win!!\n",
      "7 : 0 Win!!, 1 Lose\n",
      "8 : 0 Lose, 1 Win!!\n",
      "9 : 0 Lose, 1 Win!!\n",
      "10 : 0 Lose, 1 Win!!\n",
      "11 : 0 Lose, 1 Win!!\n",
      "12 : 0 Lose, 1 Win!!\n",
      "13 : 0 Win!!, 1 Lose\n",
      "14 : 0 Lose, 1 Win!!\n",
      "15 : 0 Lose, 1 Win!!\n",
      "16 : 0 Lose, 1 Win!!\n",
      "17 : 0 Lose, 1 Win!!\n",
      "18 : 0 Win!!, 1 Lose\n",
      "19 : 0 Win!!, 1 Lose\n",
      "20 : 0 Lose, 1 Win!!\n",
      "21 : 0 Lose, 1 Win!!\n",
      "22 : 0 Lose, 1 Win!!\n",
      "23 : 0 Win!!, 1 Lose\n",
      "24 : 0 Win!!, 1 Lose\n",
      "25 : 0 Win!!, 1 Lose\n",
      "26 : 0 Win!!, 1 Lose\n",
      "27 : 0 Win!!, 1 Lose\n",
      "28 : 0 Win!!, 1 Lose\n",
      "29 : 0 Win!!, 1 Lose\n",
      "30 : 0 Win!!, 1 Lose\n",
      "31 : 0 Win!!, 1 Lose\n",
      "32 : 0 Win!!, 1 Lose\n",
      "33 : 0 Win!!, 1 Lose\n",
      "34 : 0 Win!!, 1 Lose\n",
      "35 : 0 Win!!, 1 Lose\n",
      "36 : 0 Win!!, 1 Lose\n",
      "37 : 0 Win!!, 1 Lose\n",
      "38 : 0 Win!!, 1 Lose\n",
      "39 : 0 Lose, 1 Win!!\n",
      "40 : 0 Win!!, 1 Lose\n",
      "41 : 0 Win!!, 1 Lose\n",
      "42 : 0 Win!!, 1 Lose\n",
      "43 : 0 Win!!, 1 Lose\n",
      "44 : 0 Win!!, 1 Lose\n",
      "45 : 0 Win!!, 1 Lose\n",
      "46 : 0 Win!!, 1 Lose\n",
      "47 : 0 Win!!, 1 Lose\n",
      "48 : 0 Win!!, 1 Lose\n",
      "49 : 0 Win!!, 1 Lose\n",
      "50 : 0 Win!!, 1 Lose\n",
      "51 : 0 Win!!, 1 Lose\n",
      "52 : 0 Win!!, 1 Lose\n",
      "53 : 0 Win!!, 1 Lose\n",
      "54 : 0 Win!!, 1 Lose\n",
      "55 : 0 Win!!, 1 Lose\n",
      "56 : 0 Win!!, 1 Lose\n",
      "57 : 0 Win!!, 1 Lose\n",
      "58 : 0 Win!!, 1 Lose\n",
      "59 : 0 Win!!, 1 Lose\n",
      "60 : 0 Win!!, 1 Lose\n",
      "61 : 0 Win!!, 1 Lose\n",
      "62 : 0 Win!!, 1 Lose\n",
      "63 : 0 Win!!, 1 Lose\n",
      "64 : 0 Win!!, 1 Lose\n",
      "65 : 0 Win!!, 1 Lose\n",
      "66 : 0 Win!!, 1 Lose\n",
      "67 : 0 Win!!, 1 Lose\n",
      "68 : 0 Win!!, 1 Lose\n",
      "69 : 0 Win!!, 1 Lose\n",
      "70 : 0 Win!!, 1 Lose\n",
      "71 : 0 Win!!, 1 Lose\n",
      "72 : 0 Win!!, 1 Lose\n",
      "73 : 0 Win!!, 1 Lose\n",
      "74 : 0 Win!!, 1 Lose\n",
      "75 : 0 Win!!, 1 Lose\n",
      "76 : 0 Win!!, 1 Lose\n",
      "77 : 0 Win!!, 1 Lose\n",
      "78 : 0 Win!!, 1 Lose\n",
      "79 : 0 Win!!, 1 Lose\n",
      "80 : 0 Lose, 1 Win!!\n",
      "81 : 0 Win!!, 1 Lose\n",
      "82 : 0 Win!!, 1 Lose\n",
      "83 : 0 Win!!, 1 Lose\n",
      "84 : 0 Win!!, 1 Lose\n",
      "85 : 0 Win!!, 1 Lose\n",
      "86 : 0 Lose, 1 Win!!\n",
      "87 : 0 Win!!, 1 Lose\n",
      "88 : 0 Win!!, 1 Lose\n",
      "89 : 0 Win!!, 1 Lose\n",
      "90 : 0 Win!!, 1 Lose\n",
      "91 : 0 Win!!, 1 Lose\n",
      "92 : 0 Win!!, 1 Lose\n",
      "93 : 0 Win!!, 1 Lose\n",
      "94 : 0 Win!!, 1 Lose\n",
      "95 : 0 Win!!, 1 Lose\n",
      "96 : 0 Win!!, 1 Lose\n",
      "97 : 0 Win!!, 1 Lose\n",
      "98 : 0 Win!!, 1 Lose\n",
      "99 : 0 Win!!, 1 Lose\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):  #試行数分繰り返す\n",
    "    state = 0\n",
    "    state_old = [0,0]\n",
    "    rewards = [0,0]\n",
    "    actions = [0,0]\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    while(1):\n",
    "        actions[0] = getAction(state, epsilon, QVs[0])\n",
    "        state_old[0] = state\n",
    "        state, rewards, done = step(state, actions[0], 0)\n",
    "        updateQValue(actions[1], rewards[1], state, state_old[1], QVs[1], gamma, alpha)\n",
    "        if (done==True):\n",
    "            updateQValue(actions[0], rewards[0], state, state_old[0], QVs[0], gamma, alpha)\n",
    "            print('{} : 0 Lose, 1 Win!!'.format(episode))\n",
    "            break\n",
    "        actions[1] = getAction(state, epsilon, QVs[1])\n",
    "        state_old[1] = state\n",
    "        state, rewards, done = step(state, actions[1], 1)\n",
    "        updateQValue(actions[0], rewards[0], state, state_old[0], QVs[0], gamma, alpha)\n",
    "        if (done==True):\n",
    "            updateQValue(actions[1], rewards[1], state, state_old[1], QVs[1], gamma, alpha)\n",
    "            print('{} : 0 Win!!, 1 Lose'.format(episode))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0\n",
      "[[-0.03203613  0.8099999  -0.02645507]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.10124999 -0.24046874 -0.18140624]\n",
      " [ 0.         -0.25312498  0.8999864 ]\n",
      " [ 0.          0.899978    0.        ]\n",
      " [ 0.899999    0.         -0.4359375 ]\n",
      " [-0.703125   -0.646875   -0.6234375 ]\n",
      " [-0.26874998 -0.39374998  0.99999905]\n",
      " [-0.4359375   0.9999999   0.        ]\n",
      " [ 1.          0.         -0.5       ]\n",
      " [-0.984375   -0.96875    -0.984375  ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Agent 1\n",
      "[[-0.7274703   0.          0.        ]\n",
      " [ 0.20309943  0.          0.        ]\n",
      " [-0.8098532  -0.80977774 -0.80984014]\n",
      " [ 0.          0.          0.5352539 ]\n",
      " [ 0.          0.590625    0.        ]\n",
      " [ 0.63984376 -0.01406249 -0.225     ]\n",
      " [-0.8999899  -0.89998716 -0.89998543]\n",
      " [ 0.          0.          0.96875   ]\n",
      " [ 0.          0.96875    -0.5       ]\n",
      " [ 0.9921875  -0.5        -0.5       ]\n",
      " [-1.         -1.         -1.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Agent 0\")\n",
    "print(QVs[0])\n",
    "print(\"Agent 1\")\n",
    "print(QVs[1])\n",
    "np.savetxt('QV0.txt', QVs[0])\n",
    "np.savetxt('QV1.txt', QVs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必勝法と同じ取り方を学習しているかの確認<br>\n",
    "Check if you are learning the same winning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0\n",
      "残り本数 11 取る数 [2] 必勝法 2 正解\n",
      "残り本数 10 取る数 [1 2 3] 必勝法 1 不定\n",
      "残り本数 9 取る数 [1] 必勝法 0 なんでもよい\n",
      "残り本数 8 取る数 [3] 必勝法 3 正解\n",
      "残り本数 7 取る数 [2] 必勝法 2 正解\n",
      "残り本数 6 取る数 [1] 必勝法 1 正解\n",
      "残り本数 5 取る数 [3] 必勝法 0 なんでもよい\n",
      "残り本数 4 取る数 [3] 必勝法 3 正解\n",
      "残り本数 3 取る数 [2] 必勝法 2 正解\n",
      "残り本数 2 取る数 [1] 必勝法 1 正解\n",
      "残り本数 1 取る数 [2] 必勝法 0 なんでもよい\n",
      "Agent 1\n",
      "残り本数 11 取る数 [2 3] 必勝法 2 不定\n",
      "残り本数 10 取る数 [1] 必勝法 1 正解\n",
      "残り本数 9 取る数 [2] 必勝法 0 なんでもよい\n",
      "残り本数 8 取る数 [3] 必勝法 3 正解\n",
      "残り本数 7 取る数 [2] 必勝法 2 正解\n",
      "残り本数 6 取る数 [1] 必勝法 1 正解\n",
      "残り本数 5 取る数 [3] 必勝法 0 なんでもよい\n",
      "残り本数 4 取る数 [3] 必勝法 3 正解\n",
      "残り本数 3 取る数 [2] 必勝法 2 正解\n",
      "残り本数 2 取る数 [1] 必勝法 1 正解\n",
      "残り本数 1 取る数 [1 2 3] 必勝法 0 なんでもよい\n"
     ]
    }
   ],
   "source": [
    "for j in range(2):\n",
    "    print(\"Agent\", j)\n",
    "    for i in range(BOTTLE_N):\n",
    "        a = np.where(QVs[j][i]==QVs[j][i].max())[0]\n",
    "        print('残り本数',BOTTLE_N-i,'取る数',a+1,'必勝法',(BOTTLE_N-i-1)%4,'なんでもよい' if (BOTTLE_N-i-1)%4 == 0 else \\\n",
    "              ('不定' if a.size >1 else ('正解' if (BOTTLE_N-i-1)%4 == a+1 else '不正解')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
