{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1人の意思決定（ミントタブレット問題）\n",
    "\n",
    "・強化学習の1つであるQラーニングを用いる<br>\n",
    "・エージェントは1つだけ用いる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態遷移：状態を変化させるための関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    reward = 0\n",
    "    if state==0:#閉じている\n",
    "        if action==0:#開ける\n",
    "            state = 1\n",
    "    elif state==1:#開いていて，ミント菓子がある\n",
    "        if action==1:#閉じる\n",
    "            state = 0\n",
    "        elif action==2:#傾ける\n",
    "            state = 2\n",
    "            reward = 1\n",
    "    else:#開いていて，ミント菓子がない\n",
    "        if action==1:\n",
    "            state = 0\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行動選択：Q値から次の行動を選択するための関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(state, epsilon, qv):\n",
    "    if epsilon > np.random.uniform(0, 1):#徐々に最適行動のみをとる、ε-greedy法\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    else:\n",
    "        a = np.where(qv[state]==qv[state].max())[0]\n",
    "        next_action = np.random.choice(a)\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q値の更新：状態，行動，報酬，次の状態を用いてQ値を更新するための関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQValue(qv, state, action, reward, next_state):\n",
    "    gamma = 0.9\n",
    "    alpha = 0.5\n",
    "    next_maxQ=max(qv[next_state])\n",
    "    qv[state, action] = (1 - alpha) * qv[state, action] + alpha * (reward + gamma * next_maxQ)\n",
    "    return qv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 5  #総試行回数\n",
    "num_steps = 10  #1試行の中の行動数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "1 1 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "1 0 0\n",
      "1 1 0\n",
      "0 2 0\n",
      "0 2 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "episode : 1 total reward 0\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "0 1 0\n",
      "0 2 0\n",
      "0 1 0\n",
      "0 1 0\n",
      "0 2 0\n",
      "0 1 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "1 2 1\n",
      "episode : 2 total reward 1\n",
      "[[0.  0.  0. ]\n",
      " [0.  0.  0.5]\n",
      " [0.  0.  0. ]]\n",
      "0 2 0\n",
      "0 0 0\n",
      "1 2 1\n",
      "2 0 0\n",
      "2 1 0\n",
      "0 0 0\n",
      "1 2 1\n",
      "2 1 0\n",
      "0 0 0\n",
      "1 2 1\n",
      "episode : 3 total reward 3\n",
      "[[0.63925312 0.         0.        ]\n",
      " [0.         0.         1.0741875 ]\n",
      " [0.         0.253125   0.        ]]\n",
      "0 0 0\n",
      "1 2 1\n",
      "2 1 0\n",
      "0 0 0\n",
      "1 2 1\n",
      "2 1 0\n",
      "0 0 0\n",
      "1 2 1\n",
      "2 1 0\n",
      "0 1 0\n",
      "episode : 4 total reward 3\n",
      "[[1.04250601 0.46912771 0.        ]\n",
      " [0.         0.         1.44350257]\n",
      " [0.         0.79798454 0.        ]]\n",
      "0 0 0\n",
      "1 2 1\n",
      "2 1 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "1 2 1\n",
      "2 1 0\n",
      "0 0 0\n",
      "1 2 1\n",
      "2 1 0\n",
      "episode : 5 total reward 3\n",
      "[[1.41657498 0.46912771 0.        ]\n",
      " [0.71137995 0.         1.8244514 ]\n",
      " [0.         1.16070386 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "QV = np.zeros((3, 3))\n",
    "for episode in range(num_episodes):  #試行数分繰り返す\n",
    "    state = 0#初期状態に戻す\n",
    "    sum_reward = 0#累積報酬\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    for t in range(num_steps):  #1試行のループ\n",
    "        action = getAction(state, epsilon, QV)    # a_{t+1} \n",
    "        next_state, reward = step(state, action)\n",
    "        print(state, action, reward)\n",
    "        sum_reward += reward  #報酬を追加\n",
    "        QV = updateQValue(QV, state, action, reward, next_state)\n",
    "        state = next_state\n",
    "    print('episode : %d total reward %d' %(episode+1, sum_reward))\n",
    "    print(QV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
