{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1人の意思決定（ミントタブレット問題）<br>\n",
    "# Decision-Making by One Person (Mint Tablet Problem)<br>\n",
    "<br>\n",
    "\n",
    "・強化学習の1つであるQラーニングを用いる<br>\n",
    "・This time, we use O-learning, which is one of the reinforcement learning.<br>\n",
    "<br>\n",
    "・エージェントは1つだけ用いる<br>\n",
    "・We use only one agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態遷移：状態を変化させるための関数<br>\n",
    "State transition: The function used to change state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    reward = 0\n",
    "    if state==0:#閉じている closed\n",
    "        if action==0:#開ける open\n",
    "            state = 1\n",
    "    elif state==1:#開いていて，ミント菓子がある opened with mint\n",
    "        if action==1:#閉じる close\n",
    "            state = 0\n",
    "        elif action==2:#傾ける tip\n",
    "            state = 2\n",
    "            reward = 1\n",
    "    else:#開いていて，ミント菓子がない opend without mint\n",
    "        if action==1:#閉じる close\n",
    "            state = 0\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行動選択：Q値から次の行動を選択するための関数<br>\n",
    "Select actions: The function to select next action from Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(state, epsilon, qv):\n",
    "    #徐々に最適行動のみをとる、ε-greedy法\n",
    "    #gradually take the optimal behavior at that time, ε-greedy method\n",
    "    if epsilon > np.random.uniform(0, 1):\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    else:\n",
    "        a = np.where(qv[state]==qv[state].max())[0]\n",
    "        #その時点での最適行動が複数ある場合はランダムに選択\n",
    "        #If there are multiple optimal actions at that time, select randomly from them\n",
    "        next_action = np.random.choice(a)\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q値の更新：状態，行動，報酬，次の状態を用いてQ値を更新するための関数<br>\n",
    "Update Q-value: The function to update Q-value based on the current state, the behavior, the reward and the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQValue(qv, state, action, reward, next_state, gamma, alpha):\n",
    "    next_maxQ=max(qv[next_state])\n",
    "    qv[state, action] = (1 - alpha) * qv[state, action] + alpha * (reward + gamma * next_maxQ)\n",
    "    return qv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数の設定 Settings of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 5  #総試行回数 total number of trials\n",
    "num_steps = 10  #1試行の中の行動数 number of actions in one trial\n",
    "gamma = 0.9  #割引率 discount rate\n",
    "alpha = 0.5  #学習係数 learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の実行 Do reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "episode : 1 total reward 2\n",
      "[[0.225   0.      0.     ]\n",
      " [0.225   0.      0.75   ]\n",
      " [0.      0.10125 0.     ]]\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 0  reward: 0\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "episode : 2 total reward 2\n",
      "[[0.8413875  0.         0.        ]\n",
      " [0.225      0.2025     1.09696875]\n",
      " [0.0455625  0.46485141 0.        ]]\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "episode : 3 total reward 3\n",
      "[[1.27756212 0.         0.        ]\n",
      " [0.225      0.2025     1.56134104]\n",
      " [0.0455625  0.90863222 0.        ]]\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "episode : 4 total reward 3\n",
      "[[1.63964605 0.         0.        ]\n",
      " [0.225      0.2025     1.93823726]\n",
      " [0.0455625  1.27715335 0.        ]]\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 1  reward: 0\n",
      "state: 0  action: 0  reward: 0\n",
      "state: 1  action: 2  reward: 1\n",
      "state: 2  action: 0  reward: 0\n",
      "state: 2  action: 1  reward: 0\n",
      "episode : 5 total reward 3\n",
      "[[1.85123237 0.         0.        ]\n",
      " [0.225      0.2025     2.24851766]\n",
      " [0.69534174 1.580344   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "QV = np.zeros((3, 3))\n",
    "for episode in range(num_episodes):  #試行数分繰り返す loop for the number of trials\n",
    "    state = 0#初期状態に戻す reset to initial state\n",
    "    sum_reward = 0#累積報酬 cumulative reward\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    for t in range(num_steps):  #1試行のループ loop for one trial\n",
    "        action = getAction(state, epsilon, QV)    # a_{t+1} \n",
    "        next_state, reward = step(state, action)\n",
    "        print(\"state:\", state, \" action:\", action, \" reward:\", reward)\n",
    "        sum_reward += reward  #報酬を追加 add reward\n",
    "        QV = updateQValue(QV, state, action, reward, next_state, gamma, alpha)\n",
    "        state = next_state\n",
    "    print('episode : %d total reward %d' %(episode+1, sum_reward))\n",
    "    print(QV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
